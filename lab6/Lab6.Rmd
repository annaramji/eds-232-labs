---
title: "Lab6"
author: Anna Ramji
date: "2023-03-06"
output: html_document
---

## Case Study: Eel Distribution Modeling

This week's lab follows a project modeling the eel species Anguilla australis described by Elith et al. (2008). There are two data sets for this lab.  You'll use one for training and evaluating your model, and you'll use your model to make predictions predictions on the other.  Then you'll compare your model's performance to the model used by Elith et al.

(presence/absence data)

```{r setup}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
```


```{r library}
library(tidyverse)
library(here)
library(tidymodels)
library(ranger)
library(ggpmisc)
library(vip)
library(xgboost) #package for boosted trees
library(patchwork)
library(gt)
library(dials)
library(kableExtra)

```


## Data

Grab the training data sets (eel.model.data.csv, eel.eval.data.csv) from github here:
https://github.com/MaRo406/eds-232-machine-learning/blob/main/data 

```{r}
eel_eval_data <- read_csv(here("lab6", "data", "eel.eval.data.csv"))

eel_model_data <- read_csv(here("lab6", "data", "eel.model.data.csv")) |> 
  mutate(Angaus = as.factor(Angaus)) |> 
  mutate(Method = as.factor(Method)) |> 
  select(-Site) # dropping site, because it's essentially an index column
```


### Split and Resample

Split the model data (eel.model.data.csv) into a training and test set, stratified by outcome score (Angaus). Use 10-fold CV to resample the training set.

```{r split-resample}
set.seed(123) # for reproducibility

data_split <- initial_split(eel_model_data, prop = .7, strata = Angaus) #split data stratified by survived

train <- training(data_split)#get training data
test <- testing(data_split) #get testing data

cv_folds <- vfold_cv(train, v = 10)

```


### Preprocess

Create a recipe to prepare your data for the XGBoost model

```{r xgboost-recipe}
# define recipe
recipe <- recipe(Angaus ~ .,
                data = train) |>  #create model recipe
  step_dummy(all_nominal_predictors()) |>  #create dummy variables from all factors
  step_normalize(all_numeric_predictors()) #normalize all numeric predictors

```

## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined in lecture, first we conduct tuning on just the learning rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r model}

# set model
xgb_model <- boost_tree(learn_rate = tune()) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

# set workflow
xgb_workflow <- workflow() |>  #create workflow
  add_model(xgb_model) |>  #add boosted trees model
  add_recipe(recipe) #add recipe

```


2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run. You could use {tictoc} or Sys.time().

```{r grid}
# Mateo said we could have this part outside of the 
learn_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

system.time(
  xgb_cv_tune <- tune_grid(xgb_workflow,
                           resamples = cv_folds,
                           grid = learn_grid
                           # ,
                         #  metrics = metric_set(accuracy)
                           ) 
)

```


3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r performance}
xgb_best <- show_best(xgb_cv_tune, n = 3,
                      metric = "roc_auc") # get metrics for best random forest model
# Mateo said to use roc_auc instead of accuracy since it is more meaningful 

# xgb_best |> kableExtra::kbl() |> 
#   kableExtra::kable_classic()

xgb_best |> gt::gt() |> fmt_number(columns = c(learn_rate, mean, std_err),
                                   decimals = 4)
```


### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

```{r learning-rate-set}
learn_best <- select_best(xgb_cv_tune,
                          metric = "roc_auc") # Mateo said to select roc_auc as it is a metric that gives more information than accuracy
learn_rate <- learn_best$learn_rate

# set model
xgb_model_tune_tree <- boost_tree(learn_rate = learn_rate,
                                  trees = tune(),
                                  tree_depth = tune(),
                                  min_n = tune(),
                                  loss_reduction = tune()) |> 
  set_engine("xgboost") |> 
  set_mode("classification")
  
#xgb_final <- finalize_workflow(xgb_workflow, select_best(xgb_cv_tune, metric = "roc_auc"))

# set workflow
xgb_workflow_tree <- workflow() |>  #create workflow
  add_model(xgb_model_tune_tree) |>  #add boosted trees model
  add_recipe(recipe) #add recipe

```


2.  Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space

```{r latin-hypercube}
latin_hypercube_trees <- grid_latin_hypercube(
                                        trees(),
                                        tree_depth(),
                                        min_n(),
                                        loss_reduction(),
                                        size = 50 # number of parameter combinations returned
                                        )


system.time(
  xgb_cv_tune_trees <- tune_grid(xgb_workflow_tree,
                           resamples = cv_folds,
                           grid = latin_hypercube_trees #, 
                          # metrics = metric_set(accuracy)
                           ) #use cross validation to tune learn_rate and trees parameters
)

```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r trees-performance}

xgb_best_trees <- show_best(xgb_cv_tune_trees, n = 3, metric = "roc_auc")

xgb_best_trees |> gt::gt()

```


### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

```{r tune-stochastic1}
trees_params <- select_best(xgb_cv_tune_trees, metric = "roc_auc")
trees <- trees_params$trees
tree_depth <- trees_params$tree_depth
min_n <- trees_params$min_n
loss_reduction <- trees_params$loss_reduction


# set model
xgb_model_tune_stochastic <- boost_tree(
  learn_rate = learn_rate,
  # tree parameters
  trees = trees,
  tree_depth = tree_depth,
  min_n = min_n,
  loss_reduction = loss_reduction,
  # stochastic parameters
  mtry = tune(),
  sample_size = tune()
  ) |> 
  set_engine("xgboost") |> 
  set_mode("classification")
  
#xgb_final <- finalize_workflow(xgb_workflow, select_best(xgb_cv_tune, metric = "roc_auc"))

# set workflow
xgb_workflow_stochastic <- workflow() |>  #create workflow
  add_model(xgb_model_tune_stochastic) |>  #add boosted trees model
  add_recipe(recipe) #add recipe


```
 

2.  Set up a tuning grid. Use grid_latin_hypercube() again.

```{r tune-stochastic}

latin_hypercube_stochastic <- grid_latin_hypercube(
                                        sample_size = sample_prop(),
                                        finalize(mtry(), train),
                                        size = 30
                                        )

system.time(
  xgb_cv_tune_stochastic <- tune_grid(xgb_workflow_stochastic,
                           resamples = cv_folds,
                           grid = latin_hypercube_stochastic
                         #  , metrics = metric_set(accuracy)
                           ) # tune based on accuracy) #use cross validation to tune learn_rate and trees parameters
)



```



3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r stochastic-performance}

xgb_best_stochastic <- show_best(xgb_cv_tune_stochastic, n = 3, metric = "roc_auc") #get metrics for best random forest model

xgb_best_stochastic |> gt::gt() |> fmt_number(columns = c(sample_size, mean, std_err), decimals = 4)


autoplot(xgb_cv_tune_stochastic)

```


## Finalize workflow and make final prediction

```{r finalize-workflow}
# best_stochastic <- select_best(xgb_cv_tune_stochastic, metric = "roc_auc")
# 
# mtry <- best_stochastic$mtry
# sample_size <- best_stochastic$sample_size

# # set model
# xgb_model_final <- boost_tree(
#   learn_rate = learn_rate,
#   # tree parameters
#   trees = trees,
#   tree_depth = tree_depth,
#   min_n = min_n,
#   loss_reduction = loss_reduction,
#   # stochastic parameters
#   mtry = mtry,
#   sample_size = sample_size
#   ) |> 
#   set_engine("xgboost") |> 
#   set_mode("classification")
  
#xgb_final <- finalize_workflow(xgb_workflow, select_best(xgb_cv_tune, metric = "roc_auc"))

# set workflow
# xgb_workflow_final <- workflow() |>  #create workflow
#   add_model(xgb_model_final) |>  #add boosted trees model
#   add_recipe(recipe) #add recipe

xgb_final_wf <- finalize_workflow(xgb_workflow_stochastic,
                                    select_best(xgb_cv_tune_stochastic,
                                                metric = "roc_auc"))

```


```{r fit-predict-train}

# fitting and predicting ----
train_fit <- fit(xgb_final_wf, train)

test_predict <- predict(train_fit, test) |> 
  bind_cols(test)

accuracy(test_predict, truth = Angaus, estimate = .pred_class) #get accuracy of testing prediction

# using last fit ----

last_train_fit <- last_fit(xgb_final_wf, split = data_split)
# collect metrics 

model_performance1 <- last_train_fit |> collect_metrics() 
model_performance1 |> gt()


test_predict %>% 
  conf_mat(truth = Angaus, estimate = .pred_class) %>% #create confusion matrix
  autoplot(type = "heatmap") + #plot confusion matrix with heatmap
  theme_bw() + #change theme
 # theme(axis.text.x = element_text(angle = 30, hjust=1)) +
  #rotate axis labels
  labs(title = "Final Prediction on Model Data")


```


1.  How well did your model perform? What types of errors did it make?

My model is better than a baseline of guessing that the eel species Angaus is either always present or never present (always absent). It is correctly guessing absence fairly well, but it is not particularly better than a random guess for predicting presence of Angaus. 


## Fit your model the evaluation data and compare performance

1.  Now use your final model to predict on the other dataset (eval.data.csv)

```{r fit-predict-eval}

# we already dropped the "Site" column from the eel_model_data, which is important for this 
eel_eval_tidy <- eel_eval_data |> 
  rename(Angaus = Angaus_obs) |> 
  mutate(Angaus = as.factor(Angaus)) |> 
  mutate(Method = as.factor(Method))

model_eval_pred <- predict(train_fit, eel_eval_data) |> 
  bind_cols(eel_eval_tidy)

#collect_metrics(model_eval_pred)

accuracy(model_eval_pred, truth = Angaus, estimate = .pred_class) #get accuracy of testing prediction

model_eval_pred %>% 
  conf_mat(truth = Angaus, estimate = .pred_class) %>% #create confusion matrix
  autoplot(type = "heatmap") + #plot confusion matrix with heatmap
  theme_bw() + #change theme
 # theme(axis.text.x = element_text(angle = 30, hjust=1)) +
  #rotate axis labels
  labs(title = "Performance on Evaluation Set")



```


2.  How does your model perform on this data?

My model performs similarly on this data to the performance on the model training data -- it is still fairly good at predicting absence of the eels, but it is not particulalry better than random chance at predicting presence of the eels. The true negative rate is high, but the true positive and false positive rates appear to be roughly equal. 

3.  How do your results compare to those of Elith et al.?

-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?


```{r}
train_fit |> 
  extract_fit_parsnip() |> 
  vip(num_features = 12L) +
  theme_bw() +
  labs(title = "Variable Importance for Model Evaluation") 

```

My top variables of importance include the same as the researchers': Summer air temperature (°C) (`SegSumT`), which is tied for first place in my model with `DSDist` or Distance to the coast (in km) and `USNative`, or Area with indigenous forest (proportion). Elith's second most important variable is `USNative`, or Area with indigenous forest (proportion), at 11.3%, and is tied for first place in my model, at around 13%.
Another discrepancy here between my model and Elith et al's model variable importance results is that my second-most important variable is followed by Winter air temperature (°C), normalized with respect to SegJanT (`SegTSeas`) for my model, which only has a relative contribution of 5.7%. The level of importance for the top variable (`SegSumT`) is also lower in my model, at around 15% vs. the Elith's 24.7%.  Our 10th most important variable is Days per month with rain >25 mm (`USRainDays`), which is the 7th most important in Elith et al's study. Our 6th most important variable is `USAvgT`, or Average temperature in catchment (°C) compared with segment, normalized with respect to SegJanT, at around 8% importance, and is the 9th most important variable in Elith et al's model (5.7%). 

Overall, the variable important results tell me that the distribution of this eel species is likely related or dependent on temperature and precipitation -- more specifically, Summer air temperature, Winter air temperature, precipitation levels in an area, as well as the habitat or cover provided by the presence of indigenous forests, and the distance to the ocean. 
