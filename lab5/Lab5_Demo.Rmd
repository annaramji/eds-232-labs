---
title: "Lab5_Demo"
author: "Mateo Robbins"
date: "2024-02-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)    
library(ggplot2) #great plots
library(rsample)  #data splitting 
library(recipes) #data preprocessing
library(skimr) #data exploration
library(tidymodels) #re-entering tidymodel mode
library(kknn) #knn modeling
```

###k-nearest neighbor in tidymodels

## Data

```{r data}
data(attrition)
churn <- attrition %>% mutate_if(is.ordered, .funs = factor, ordered = F) 
#skim(churn) run in console
```

Not doing the data exploration here in the interest of time and since we are familiar with this dataset.

```{r initial_split}
set.seed(808)
#initial split of data, default 75/25
churn_split <- initial_split(churn, 0.75)
churn_test <- testing(churn_split)
churn_train <- training(churn_split)
```

We need to create a recipe and do the preprocessing by dummy coding the nominal variables and standardizing the numeric variables.

```{r recipe}
#preprocessing
knn_rec <- recipe(Attrition ~ . ,
                  data = churn_train) |>
  step_dummy(all_nominal(),
             -all_outcomes(), # just predictors
             one_hot = TRUE # type of dummy encoding
             # e.x. cities --> string --> split single variable into set of features (one feature for each city, e.g., one column for chicago, get a 0 or 1 for each of those dimensions)
             ) |>
  step_normalize(all_numeric(), 
                 -all_outcomes()
                 # centering, convert mean to 0, dividing out SD so they're all put on a similar scale
                 ) |>  # sensitive to scale ?
  prep()

# knn_rec
# single outcome, 30 predictors
# prepped parameters for the centering and scaling, calculated on the training data 

baked_train <- bake(knn_rec, churn_train)

```

Recall: if you want to see the what the recipe is doing to your data, you can first prep() the recipe to estimate the parameters needed for each step and then bake(new_data = NULL) to pull out the training data with those steps applied.

Now the recipe is ready to be applied to the test data.

```{r bake_test}
baked_test <- bake(knn_rec, churn_test)

# baked_test
```

##Specify the k-nearest neighbor model

```{r knn_spec}
# specify our model
knn_spec <- nearest_neighbor(neighbors = 5) |>
  set_engine("kknn") |> # k nearest neighbor engine
  # method of estimation
  set_mode("classification") # specify if classification task or a regression task


```

```{r knn_fit}
# fit model 

knn_fit <- knn_spec |>
  fit(Attrition ~ . , 
      data = churn_train)

knn_fit
```

- minimal misclassification -- 80% accuracy, 20% misclassification
- we did this step when we just decided to use k=5

(naive way to do this kind of modeling)

starting over, doing it the real way (finding the best value of k)

```{r cv}
set.seed(808) # setting it again bc chunk-specific, first iteration of that set randomness, cv splitting data again, to achieve that same split, we need to set that seed again

# if you copied and pasted all of this code twice, if set in global enviro vs in chunk, iteration of that pseudo randomness is at a different position vs. if u set in chunk for each step 

# 5-fold CV on the training dataset (instead of 10 (typically best practice) for in-class demo)
cv_folds <- churn_train |>
  vfold_cv(v = 5)

```

We now have a recipe for processing the data, a model specification, and CV splits for the training data.

Let's put it all together in a workflow.

```{r knn_workflow}

knn_workflow <- workflow() |>
  add_model(knn_spec) |>
  add_recipe(knn_rec)


```
Let's fit the resamples and carry out the cross-validation
```{r knn_res}

knn_res <- knn_workflow |>
  fit_resamples(
    resamples = cv_folds,
    control = control_resamples(save_pred = TRUE) # save prediction from each fold
  )

```

```{r}
# Check the performance
knn_res |>
  collect_metrics()

```

- we ran the model 5x on each of the 5 folds, calculated accuracy and roc_Area under curve
- trains on 4, tests on 1 --> accuracy
- values for performance metrics
- taking the performance from each run, averaging across to 



Let's find the best value of k
```{r spec_with_tuning}
# Define our KNN model with tuning
knn_spec_tune <- nearest_neighbor(neighbors = tune()) |> # specify model again, rather than telling it we want u to use 5 neighbors, telling it we're going to tune the model to get the best value of that parameter (nn)
  set_mode("classification") |>
  set_engine("kknn")
  

  
```

```{r wf_knn_tune}
# Define a new workflow
wf_knn_tune <- workflow() |>
  add_model(knn_spec_tune) |>
  add_recipe(knn_rec)


```

This time before we fit the model we need to tell R which values to try for the parameter that we're tuning.

To tune our hyperparameter(s), we will use the tune_grid() function (instead of the fit() or fit_resamples() functions).

This tune_grid() is similar to fit_resamples() except that it takes an additional argument: grid. We will pass the possible values of our hyperparameter(s) to this grid argument, and it will evaluate each fold of our sample on each set of hyperparameters passed to grid.

We'll explore a few values of k: (1,5,10, 20, 30, 40, 50, 60, 70, 80, 90, 100)

```{r fit_knn_cv}
# Fit the workflow on our predefined folds and a grid of hyperparameters
fit_knn_cv <- wf_knn_tune |>
  tune_grid(
    
    cv_folds,
    grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10)))
    
  )
# running 60 times
# 5 folds, for each of 12 different sets of neighbors
# 12 diff values of k we want to try

# Check the performance with collect_metrics()
fit_knn_cv_metrics <- fit_knn_cv |>
  collect_metrics()

fit_knn_cv_metrics
max(fit_knn_cv_metrics$mean)
```

And finally, we will predict.

Use finalize_workflow() function wants (1) your initial workflow and (2) your best model.

```{r final_wf}
# The final workflow for our KNN model. Finalize_workflow takes a workflow and a set of parameters.  In this case, that set is just the best value of k
final_wf <- wf_knn_tune |>
  finalize_workflow(select_best(fit_knn_cv, metric = "accuracy"))

# Check out the final workflow object.  Choosing accuracy for interpretability in this simple binary context
final_wf
```

```{r final_fit}
# Fitting our final workflow
final_fit <- final_wf |>
  fit(data = churn_train)
# Examine the final workflow
final_fit
```

- notably better misclassification than when we arbitrarily chose 5 nearest neighbors

- improved by 5%

And finally, we can predict onto the testing dataset.

```{r churn_pred}

churn_pred <- final_fit |>
  predict(new_data = churn_test)

churn_pred |> head()

# churn_pred
```

There's a better way! You can pass your final workflow (workflow plus the best model) to the last_fit() function along with your initial split (for us: churn_split) to both (a) fit your final model on your full training dataset and (b) make predictions onto the testing dataset (defined in your initial split object).

This last_fit() approach streamlines your work (combining steps) and also lets you easily collect metrics using the collect_metrics() function

```{r last_fit}
# Write over 'final_fit' with this last_fit() approach
final_fit <- final_wf |>
  last_fit(churn_split) # takes an rsplit object

# Collect metrics on the test data!
final_fit |> collect_metrics() 
```
