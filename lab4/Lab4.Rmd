---
title: "Lab4"
author: "Anna Ramji"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(skimr)
library(tidymodels)
library(caret)
library(broom)
library(corrplot)
library(rsample)

set.seed(123) # for reproducibility 
```

## Lab 4: Fire and Tree Mortality

The database we'll be working with today includes 36066 observations of individual trees involved in prescribed fires and wildfires occurring over 35 years, from 1981 to 2016. It is a subset of a larger fire and tree mortality database from the US Forest Service (see data description for the full database here: [link](https://www.nature.com/articles/s41597-020-0522-7#Sec10)). Our goal today is to predict the likelihood of tree mortality after a fire.

### Data Exploration

Outcome variable: *yr1status* = tree status (0=alive, 1=dead) assessed one year post-fire.

Predictors: *YrFireName, Species, Genus_species, DBH_cm, CVS_percent, BCHM_m, BTL* (Information on these variables available in the database metadata ([link](https://www.fs.usda.gov/rds/archive/products/RDS-2020-0001-2/_metadata_RDS-2020-0001-2.html))).

```{r}
trees_dat<- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/trees-dat.csv")
```

> Question 1: Recode all the predictors to a zero_based integer form

```{r}
set.seed(123)

trees_dat <- trees_dat |> 
  select(-...1) |> 
  janitor::clean_names()   # to follow tidyverse style guide


trees_rec <- recipe(yr1status ~., data = trees_dat) |> 
  step_integer(c(yr_fire_name, species, genus_species) ,
               zero_based = TRUE) # zero based for all non-previously-integers

trees_prep <- prep(trees_rec)

baked_trees <- bake(trees_prep, new_data = NULL)
```


### Data Splitting

> Question 2: Create trees_training (70%) and trees_test (30%) splits for the modeling

```{r}
set.seed(123)  # for reproducibility 

trees_split <- initial_split(baked_trees, prop = 0.7) # specifying 70:30 split
trees_train <- training(trees_split) # train
trees_test  <- testing(trees_split) # test
```



> Question 3: How many observations are we using for training with this split?

```{r}
print(paste("We are working with",
            nrow(trees_train),
            "observations in the training set"))
```




### Simple Logistic Regression 

Let's start our modeling effort with some simple models: one predictor and one outcome each.

> Question 4: Choose the three predictors that most highly correlate with our outcome variable for further investigation.

```{r}
cor_trees <- cor(baked_trees)

# Make a correlation plot between the variables
corrplot(cor_trees, method = "shade",
         shade.col = NA,
         tl.col = "black",
         tl.srt = 45,
         addCoef.col = "black",
         cl.pos = "n",
         order = "original")
```


The three predictors that most highly correlate with our outcome variable are: 

- BCHM_M (`bchm_m`): Maximum bark char (also called bole char, bole scorch in other publications) vertical height from ground on a tree bole, rounded to nearest 0.01 m (m=meters). NA = not assessed.

- DBHcm (`dbh_cm`): Diameter at Breast Height rounded to the nearest 0.1 (cm)


- CVS_percent (`cvs_percent`): Percent of the pre-fire crown volume that was scorched or consumed by fire (values 0 to 100). If measured, this is the CVS from field measurements. Otherwise it is the calculated CVS from crown length measurement, where CVS=100[(CLS(2CL_pre - CLS))/CL_pre2]. NA = not assessed.


> Question 5: Use glm() to fit three simple logistic regression models, one for each of the predictors you identified.

```{r}

trees_split <- initial_split(trees_dat, # using the unbaked, non-recoded data
                             prop = 0.7) # specifying 70:30 split
trees_train <- training(trees_split) # train
trees_test  <- testing(trees_split) # test

# bchm_m model
model_bchm <- glm(data = trees_train,
                  yr1status ~ bchm_m,
                  family = "binomial")
# dbh cm model
model_dbh <- glm(data = trees_train,
                  yr1status ~ dbh_cm,
                  family = "binomial")
# cvs percent model
model_cvs <- glm(data = trees_train,
                  yr1status ~ cvs_percent,
                  family = "binomial")

broom::tidy(model_bchm)
broom::tidy(model_dbh)
broom::tidy(model_cvs)

```



### Interpret the Coefficients 

We aren't always interested in or able to interpret the model coefficients in a machine learning task. Often predictive accuracy is all we care about.

> Question 6: That said, take a stab at interpreting our model coefficients now.

```{r coefficients}
exp(coef(model_bchm)) |> broom::tidy()

exp(coef(model_dbh)) |> broom::tidy()

exp(coef(model_cvs)) |> broom::tidy()
```
`bchm_m`:

- The odds (probability of burning / probability of (1 - burning) (aka not burning))  of tree mortality after a fire increases multiplicatively by 1.2 for every 1 additional meter increase in maximum bark char vertical height from ground on a tree bole (I think of this as height of bark char as measured from the ground up on the trunk)

`dbh_cm`:

- The odds of tree mortality after a fire increases multiplicatively by 0.94 for every 1 cm increase in tree diameter measured at breast height (decrease by a multiplicative 0.06)


`cvs_percent`:

- The odds of tree mortality after a fire increases multiplicatively by 1.08 for every 1% increase in pre-fire crown volume that was scorched or consumed by fire.


> Question 7: Now let's visualize the results from these models. Plot the fit to the training data of each model.


```{r}
# bchm_m plot
ggplot(data = trees_train,
       aes(x = bchm_m,
           y = yr1status)) +
  geom_point(alpha = 0.7) +
  stat_smooth(method = "glm",
              se = TRUE,
              method.args = list(family = "binomial")) +
  theme_bw()

# dbh_cm plot
ggplot(data = trees_train,
       aes(x = dbh_cm,
           y = yr1status)) +
  geom_point(alpha = 0.7) +
  stat_smooth(method = "glm",
              se = TRUE,
              method.args = list(family = "binomial")) +
  theme_bw()

# cvs_percent plot
ggplot(data = trees_train,
       aes(x = cvs_percent,
           y = yr1status)) +
  geom_point(alpha = 0.7) +
  stat_smooth(method = "glm",
              se = TRUE,
              method.args = list(family = "binomial")) + 
  theme_bw()

```


### Multiple Logistic Regression

Let's not limit ourselves to a single-predictor model. More predictors might lead to better model performance.

> Question 8: Use glm() to fit a multiple logistic regression called "logistic_full", with all three of the predictors included. Which of these are significant in the resulting model?

```{r}
set.seed(123)

logistic_full <- glm(yr1status ~ bchm_m + dbh_cm + cvs_percent,
                     family = "binomial",
                     data = trees_train
                     )
tidy(logistic_full)

exp(coef(logistic_full))
```
*Which of these are significant in the resulting model?*

All of our predictors appear to be significant in the multiple regression model, as they all have very low p-values ( < 0.05, significant under an alpha of 0.05).  



### Estimate Model Accuracy

Now we want to estimate our model's generalizability using resampling.

> Question 9: Use cross validation to assess model accuracy. Use caret::train() to fit four 10-fold cross-validated models (cv_model1, cv_model2, cv_model3, cv_model4) that correspond to each of the four models we've fit so far: three simple logistic regression models corresponding to each of the three key predictors (CVS_percent, DBH_cm, BCHM_m) and a multiple logistic regression model that combines all three predictors.

```{r}

# we need to convert our yr1status variable to a factor with 2 levels: 0 and 1. If we try to do this earlier on, we can't perform a lot of our steps that require the outcome variable to be numeric (such as making the correlation matrix)
baked_trees_fact <- trees_dat |> 
  mutate(yr1status = as.factor(yr1status))

set.seed(123)  # for reproducibility 

trees_split_fact <- initial_split(baked_trees_fact, prop = 0.7) # 70:30 split
trees_train_fact <- training(trees_split_fact)
trees_test_fact  <- testing(trees_split_fact)

# cvs_percent simple logistic regression model 
cv_model1 <- caret::train(yr1status ~ cvs_percent,
                          data = trees_train_fact,
                          method = "glm",
                          family = "binomial",
                          trControl = trainControl(method = "cv",
                                                   number = 10)) # 10 folds

# dbh_cm simple logistic regression model 
cv_model2 <- caret::train(yr1status ~ dbh_cm,
                          data = trees_train_fact,
                          method = "glm",
                          family = "binomial",
                          trControl = trainControl(method = "cv",
                                                   number = 10))

# bchm_m multiple logistic regression model 
cv_model3 <- caret::train(yr1status ~ bchm_m,
                          data = trees_train_fact,
                          method = "glm",
                          family = "binomial",
                          trControl = trainControl(method = "cv",
                                                   number = 10))

# multiple logistic regression model 
cv_model4 <- caret::train(yr1status ~ cvs_percent + dbh_cm + bchm_m,
                          data = trees_train_fact,
                          method = "glm",
                          family = "binomial",
                          trControl = trainControl(method = "cv",
                                                   number = 10))

```



> Question 10: Use caret::resamples() to extract then compare the classification accuracy for each model. (Hint: resamples() wont give you what you need unless you convert the outcome variable to factor form). Which model has the highest accuracy?


```{r}
# extract out of sample performance measures
summary(
  resamples(
    list(
      model1 = cv_model1, 
      model2 = cv_model2, 
      model3 = cv_model3,
      model4 = cv_model4
    )
  )
)$statistics$Accuracy
```
*Which model has the highest accuracy?*
The multiple logistic regression model (all three predictors, model4) has the highest accuracy, with a mean accuracy of 0.904, or 90.4%. 



Let's move forward with this single most accurate model.

> Question 11: Compute the confusion matrix and overall fraction of correct predictions by the model.

```{r}

# predict class
pred_class <- predict(cv_model4, trees_train_fact)

#pred_class
# create confusion matrix
confusion_matrix_train <- confusionMatrix(
  data = relevel(pred_class, ref = "1"),  # releveling so that 1 is considered positive (though emotionally tree mortality is more negative..., but I wanted the 1s to be considered positive for my analysis)
  reference = relevel(trees_train_fact$yr1status, ref = "1"))


confusion_matrix_train

```

 The overall fraction of correct predictions by the model is the "Accuracy: 0.904" which is calculated by the true positive + true negative counts divided by the total number of counts (sample size). 
 

> Question 12: Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

The matrix shows that there are far more false negatives than false positives, which could be indicative of a class imbalance, as there also more negatives (0, tree lived) over all in our original dataset. 

The "No Information Rate: 0.7169" output of the confusion matrix represents the ratio of trees that did not die vs. did die after 1 year in our training data. This means that if we guessed "0" or did not die for all of our data points, we would get an accuracy rate of 0.7169, or roughly 71.7%. 

The "Sensitivity: 0.88" metric indicates how accurately our model classifies actual events, focusing on maximizing the true positive: false negative ratio, or for the events that did occur, how many did we predict? [source: HOML Chapter 5](https://bradleyboehmke.github.io/HOML/process.html). This means that the ratio of TP / TP + FN = 0.88 for our model, which is a number we want to maximize. The specificity is the number of True Negatives divided by the number of False Negatives + True negatives. 

The 95% confidence interval indicates that we are 95% certain that interval (0.9003, 0.9076) contains the true accuracy of our model. The P value is extremely small (<0.05), which indicates that the likelihood of seeing these results if the null hypothesis were true is extremely small. 

The Detection Rate (around 0.25) is calculated by taking the number of True Positives divided by the sum of all values in the confusion matrix (TP + FP + TN + FN), and the BalancedAccuracy metric is $\frac{(sensitivity + specificity)} 2$ 


> Question 13: What is the overall accuracy of the model? How is this calculated?

The overall accuracy of the model is 0.904, or 90.4%, which is calculated by the True Positive + True Negative Counts / Total counts. 


### Test Final Model

Alright, now we'll take our most accurate model and make predictions on some unseen data (the test data).

> Question 14: Now that we have identified our best model, evaluate it by running a prediction on the test data, trees_test.

```{r evaluating}
# predict class 
predict_test <- predict(cv_model4, trees_test_fact)
# predict_test

# create confusion matrix, being sure to use test data 
confusion_matrix_test <- confusionMatrix(
  data = relevel(predict_test, ref = "1"), 
  reference = relevel(trees_test_fact$yr1status, ref = "1"))

confusion_matrix_test

```


> Question 15: How does the accuracy of this final model on the test data compare to its cross validation accuracy? Do you find this to be surprising? Why or why not?


```{r}
print(paste("The accuracy of our multiple regression model on the training data is",
            round(confusion_matrix_train$overall[1], 3),
            "or roughly",
            round(confusion_matrix_train$overall[1] * 100, 2)
            ))


print(paste("The accuracy of our multiple regression model on the test data is",
            round(confusion_matrix_test$overall[1], 3),
            "or roughly",
            round(confusion_matrix_test$overall[1] * 100, 2)
            ))

```

The cross validation accuracy for our 4th model had a mean accuracy of 0.904 (90.4%), which is slightly higher than the 0.9013 (90.13%) accuracy reported in our confusion matrix on the test data.

This is slightly surprising to me because I wasn't expecting them to be so close -- I would have expected the model to be slightly overfitted to the training data, which it technically might be as the accuracy is slightly higher for the training vs test data, but only by a fraction of a percent. On the other hand, cross validation tests for generalizability, and we got a high accuracy score from that, so it's not overly surprising that we got a similarly high number from our final model on the test data. 
