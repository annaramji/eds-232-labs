---
title: "Lab 3 Demo"
author: "Mateo Robbins"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(skimr) # for viewing data
library(glmnet) # call this to estimate our model
```

## Data Wrangling and Exploration
```{r data}
#load and inspect the data
dat <- AmesHousing::make_ames()

```

##Train a model
```{r intial_split}
# Data splitting with {rsample} 
set.seed(123) #set a seed for reproducibility
split <- rsample::initial_split(dat) # initial split on our data

ames_train <- training(split) # get training subset from split
ames_test  <- testing(split) # get testing subset from split

```

```{r model_data}
#Create training feature matrices using model.matrix() (auto encoding of categorical variables)

X <- stats::model.matrix(Sale_Price ~ ., # use "." to specify all variables in the dataset
                         data = ames_train)[,-1] # remove intercept variable (???) note: from book


# transform y with log() transformation
# predict how much a house will sell for 

Y <- log(ames_train$Sale_Price)

```

In the console:

skim(dat)

Sale_Price is the 33rd variable (?), from summary information, it looks pretty skewed, so we log transform it.

## Model Fitting
general linear model function 

fit a GLM with lasso or elasticnet regularization
```{r glmnet}
#fit a ridge model, passing X,Y,alpha to glmnet()
ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0 # 0 = ridge, 1 = LASSO
)

#plot() the glmnet model object
plot(ridge)
plot(ridge, xvar = "lambda")  
```
We can see that the model is shrinking the coefficients to near Zero


*Remember:*

- simplify model (parsimony)

- penalty term added to Sum of Squared Errors

- Lambda is dial we can use to control the penalty on the coefficients

- LASSO -- feature selection

- Ridge -- decrease big coefficients



```{r}
# lambdas applied to penalty parameter.  Examine the first few
ridge$lambda |> 
  head() # diff values of the penalty parameter

# small lambda results in large coefficients
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"),100] # 100th 

# what about for small coefficients? (increased value of lambda)
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"),1]

#coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"),1:100] 
# 100 columns of lambda, for each variable, 100 values for corresponding coefficients. Lambda goes from large to small.

```
- location seems to be important to property value
- very small coefficients with large lambda, indicates 

How much improvement to our loss function as lambda changes?

##Tuning

Cross-validation
- resampling of our data

- running our model on a portion of that data, predicting it onto another portion of our data, averaging over those folds

cv.glmnet():
"Does k-fold cross-validation for glmnet, produces a plot, and returns a value for lambda (and gamma if relax=TRUE)"
```{r cv.glmnet}
# Apply CV ridge regression to Ames data.  Same arguments as before to glmnet()
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0 # ridge
)

# Apply CV lasso regression to Ames data
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1 # LASSO
  # default nfolds = 10
)
  
# plot results
par(mfrow = c(1, 2)) # {graphics}, set or query graphical parameters
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```

10-fold CV MSE for a ridge and lasso model. What's the "rule of 1 standard error"?

In both models we see a slight improvement in the MSE as our penalty log(Î») gets larger, suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to increase. 

- Seeing how well the model is predicting that "hold" outset (???)

- as lambda increases, the mean squared error is changing

- interpret: we want to minimize mean squared error (MSE low). What values of lambda look like they have low MSE?

- smaller values, diff ranges for each.

- hurting the model at some point when we're constraining those coefficients

- at the negative values in Ridge, seeing a decent range, increase eventually

- dotted lines telling you min and max value of lambda that 1. give you the lowest Mean Squared Error. second line tells you the -- 1 standard error rule. tradeoff b/w performance just measured by MSE and parsimony in the model. don't always just wanna rely on MSE, effectiveness of the estimate varies. pick the point for lambda that gives you the most parsimonious model that gives you the lowest MSE within 1 standard error of MSE

- second line gives u 1 standard error across all folds, within 1 SE of minimum point, selecting features out of the model (increasing parsimony (fewer variables))

- grey line shows variability in the 10 runs for each point, red line is the average


Let's examine the important parameter values apparent in the plots.
```{r}
# Ridge model ----

# minimum MSE
min(ridge$cvm) 

# lambda value at this min MSE (important to know!)
ridge$lambda.min

# 1-SE rule (MSE)
ridge$cvm[ridge$lambda == ridge$lambda.1se]

# lambda for this MSE
ridge$lambda.1se


# Lasso model ----

# minimum MSE
min(lasso$cvm)

# lambda for this min MSE
ridge$lambda.min

# 1-SE rule
lasso$cvm[lasso$lambda == lasso$lambda.1se]

# lambda for this MSE
lasso$lambda.1se

# No. of coef | 1-SE MSE (makes sense because with LASSO, we're performing feature selection)
lasso$nzero[lasso$lambda == lasso$lambda.1se]
```

```{r}
# Ridge model
ridge_min <-

# Lasso model
lasso_min


par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```

