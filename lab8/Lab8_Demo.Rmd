---
title: "Lab 8 Demo"
author: "Mateo Robbins"
date: "2024-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(dplyr)
library(kernlab)
library(tidyverse)
```


```{r}
#Create simulate training data for our SVM exercise
set.seed(1)

sim_data <- tibble(
  # parameters
  # binary outcome variable
  # use 2 variables to generate data
  x1 = rnorm(40),
  x2 = rnorm(40),
  
  y = factor(rep(c(-1, 1), 20))) |> 
    # when y =1 , add offset to x1, else, keep randomly generated value
    mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),
           x2 = ifelse(y==1, x2 + 1.5, x2))
  

```

```{r}
# plot to see the structure of the data we created

ggplot(sim_data, aes(x1, x2, color = y)) +
  geom_point() +
  theme_bw()

```

```{r svm_rec}
# specify a recipe where we center to mean of 0 and scale to sd of 1
svm_rec <- recipe(y ~ ., data = sim_data) |> 
  step_center(all_predictors()) |> 
  step_scale(all_predictors())

```

```{r svm_spec}
# Create linear SVM model specification
svm_linear_spec <- svm_poly(degree = 1,
                            cost = 10) |> 
  set_mode("classification") |> 
  set_engine("kernlab")

```

In SVM, the cost parameter influences the width of the margin around the separating hyperplane. A smaller C allows a wider margin but more misclassifications are allowed. Recall that we can improve  generalization by accepting more errors on the training set. A larger C aims for a narrower margin that tries to correctly classify as many training samples as possible, even if it means a more complex model.

```{r}
# Bundle into workflow
svm_workflow <- workflow() |> 
  add_recipe(svm_rec) |> 
  add_model(svm_linear_spec)

```

```{r}
# Fit workflow

svm_linear_fit <- fit(svm_workflow, data = sim_data)

```

```{r}
# Plot the fit from kernlab engine

svm_linear_fit |> 
  extract_fit_engine() |> 
  plot()

```

decision boundary shown in white

2 values of factor y

only showing support vector -- points near the decision line

more darkly colored region = more certain the model is about the classification

arbitrarily chose the value cost = 10, now we're going to try tuning to find the best cost value

```{r tune}
svm_linear_tune <- svm_poly(degree = 1,
                            cost = tune()) |> 
  set_mode("classification") |> 
  set_engine("kernlab")

# As usual we want to tune our hyperparameter values

svm_linear_wf <- workflow() |> 
  add_model(svm_linear_tune |> 
  set_args(cost_tune())) |> 
  # shortcutting recipe
  add_formula(y~.)

set.seed(1234)
sim_data_fold <- vfold_cv(data = sim_data, strata = y)

param_grid <- grid_regular(cost(),
                           levels = 10) # how many different values for cost we want to try

# set up tuning results
tune_res <- tune_grid(
  svm_linear_wf,
  resamples = sim_data_fold,
  grid = param_grid
)

autoplot(tune_res)
```

note: when u use grid regular, it's sampling levels from paramter space, if parameter operates in non-linear fashion, it'll use non-linearly spaced parameter values

otherwise, range, divide into n equal points, but when param is not linear, grid regular will sample what seems to be in an irregular pattern -- but rly giving us the most representative sample from that irregular space 

#Finalize model and fit
```{r finalize}
# finalize model and fit
# select best cost value
best_cost <- select_best(tune_res, metric = "accuracy")

best_cost

svm_linear_final <- finalize_workflow(
  svm_linear_wf,
  best_cost
)

svm_linear_fit_final <- svm_linear_final |> 
  fit(sim_data)
svm_linear_fit_final
```

model is using best_cost for cost

20 support vectors -- 20 of the 40 data points were close enough to be considered support vectors

training error: our model got around 82% correct

- can compare to other candidate models (might try other kernels, expanding parameter search, etc. -- is it the best in the diff iterations we have, vs. RF, etc. ), or other requirements of the application


```{r sim_test}
# Create a small test data set
set.seed(2) # setting a different seed here!

# generating test data

sim_data_test <- tibble(
  # parameters
  # binary outcome variable
  # use 2 variables to generate data
  x1 = rnorm(40),
  x2 = rnorm(40),
  
  y = factor(rep(c(-1, 1), 20))) |> 
    # when y =1 , add offset to x1, else, keep randomly generated value
    mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),
           x2 = ifelse(y==1, x2 + 1.5, x2))


```

We can use augment() from {broom} to use our trained model to predict on new data (test data) and add additional info for examining model performance. 


```{r augment}
augment(svm_linear_fit, new_data = sim_data_test) |> 
  conf_mat(truth = y, estimate = .pred_class)

```

That went well, but what makes SVMs really interesting is that we can use non-linear kernels. Let us start by generating some data, but this time generate with a non-linear class boundary.

```{r}
set.seed(3) # setting new seed

sim_data2 <- tibble(
  # adding 2 to 100 of the randomly generated numbers, subtracting -2 to 50 and doing nothing to 50
  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),
  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),
  y = factor(rep(c(1, 2), c(150, 50)))
  
)

sim_data2

sim_data2 |> 
  ggplot(aes(x1, x2, color = y)) +
  geom_point() +
  theme_bw()
```

we made it so that it's not easy to split linearly (wouldn't make sense to draw a line anywhere through the data)

```{r svm_rbf}
# radial basis instead of polynomial degree 1
svm_rbf_spec <- svm_rbf() |> 
  set_mode("classification") |> 
  set_engine("kernlab")

```


```{r}
# Fit the new specification
svm_rbf_fit <- svm_rbf_spec |> 
  fit(y ~ ., data = sim_data2)

svm_rbf_fit
```

doing pretty well with the default model even though we haven't tuned anything

73 points out of our 200 are close enough to the decision boundary that they're considered support vectors

objective function value = measurement of predicted class and real class -- probabilistic estimate of is this estimate likely to be a 1 or a 2, summing across all of those predictions (??????)

```{r}
# Plot the fit
svm_rbf_fit |> 
  extract_fit_engine() |> 
  plot()

```

triangles and circles indicating classes -- 1s and 2s



```{r}
#Create the test data
set.seed(4) # new seed for different randomness

sim_data2_test <- tibble(
  # adding 2 to 100 of the randomly generated numbers, subtracting -2 to 50 and doing nothing to 50
  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),
  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),
  y = factor(rep(c(1, 2), c(150, 50)))
  
)

```

```{r}
# Examine model performance via confustion matrix

augment(svm_rbf_fit, new_data = sim_data2_test) |> 
  conf_mat(truth = y, estimate = .pred_class)

```

ROC Curves

```{r}
# We can examine our model's performance using ROC and AUC

augment(svm_rbf_fit, new_data = sim_data2_test) |> 
  roc_curve(truth = y, .pred_1) |> 
  autoplot()

```

more useful if we calculate the area under the curve

```{r}
augment(svm_rbf_fit, new_data = sim_data2_test) |> 
  roc_auc(truth = y, .pred_1)
```

